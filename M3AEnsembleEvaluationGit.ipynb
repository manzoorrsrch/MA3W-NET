{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y5v-E67JG3q",
        "outputId": "356efa27-9e9a-46e6-fc6b-ce929938afa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  8 04:19:22 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   32C    P0             54W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NU2s_gdRLc1",
        "outputId": "6ff13e0d-9a45-4d92-ba1e-81bd8dc4add4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install nibabel SimpleITK==2.3.1 einops==0.8.0 tqdm==4.67"
      ],
      "metadata": {
        "id": "Yg1QU6fzK8SY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51a340b-5c99-4720-8940-8b83f513b426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tG04OY9KIIC"
      },
      "outputs": [],
      "source": [
        "import os, sys, json, glob, random, math, time, csv\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "from tqdm import tqdm\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "import pandas as pd\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lwfVTPufLI8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5012f2-bc35-472a-fa03-be79c9a9fe68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy from memory to vm\n",
        "!cp /content/drive/MyDrive/m3a_neuroseg/cache2023_only.tar.gz /content/\n"
      ],
      "metadata": {
        "id": "jN8xT6oyYrnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip\n",
        "!tar -xzf /content/cache2023_only.tar.gz -C /content/\n"
      ],
      "metadata": {
        "id": "jbHGQpgqYtMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/cache2023_only.tar.gz\n"
      ],
      "metadata": {
        "id": "HY781K7Yd2m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use when copied into VM\n",
        "GDRIVE_ROOT=f'/content/drive/MyDrive'\n",
        "GDRIVE_PROJ_ROOT=f'{GDRIVE_ROOT}/m3a_neuroseg'\n",
        "GDRIVE_RUNS_DIR=f'{GDRIVE_PROJ_ROOT}/runs'\n",
        "\n",
        "\n",
        "DRIVE_ROOT = \"/content\"\n",
        "BRATS_ROOT = f\"/content\"          # e.g., /MyDrive/brats\n",
        "BRATS23 = f\"{BRATS_ROOT}/cache2023_only\"\n",
        "\n",
        "PROJ_ROOT = f\"{DRIVE_ROOT}/m3a_neuroseg\"    # project home\n",
        "os.makedirs(PROJ_ROOT, exist_ok=True)\n",
        "IDX_DIR = f\"{PROJ_ROOT}/data_index\"; os.makedirs(IDX_DIR, exist_ok=True)\n",
        "\n",
        "CACHE_DIR = f\"{BRATS_ROOT}/cache2023_only\";\n",
        "#os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "RUNS_DIR = f\"{PROJ_ROOT}/runs\"; os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "FIGS_DIR = f\"{PROJ_ROOT}/figs\"; os.makedirs(FIGS_DIR, exist_ok=True)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "print(\"Ready. Roots:\\n\", BRATS23, \"\\n\", \"\\n\", PROJ_ROOT,'\\n',CACHE_DIR,'\\n',RUNS_DIR)"
      ],
      "metadata": {
        "id": "Tf1u2mUyMzRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a088f9c-5c21-41bb-9b63-a27cb8c14d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready. Roots:\n",
            " /content/cache2023_only \n",
            " \n",
            " /content/m3a_neuroseg \n",
            " /content/cache2023_only \n",
            " /content/m3a_neuroseg/runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copy data indexes and splits\n",
        "!cp /content/drive/MyDrive/m3a_neuroseg/data_index/brats2023_index.csv /content/m3a_neuroseg/data_index\n",
        "\n",
        "!cp /content/drive/MyDrive/m3a_neuroseg/data_index/splits_5fold_seed42.json /content/m3a_neuroseg/data_index"
      ],
      "metadata": {
        "id": "tKx8nV2RyKgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index_path = \"/content/drive/MyDrive/m3a_neuroseg/data_index/brats2023_index.csv\"\n",
        "index_path = \"/content/m3a_neuroseg/data_index/brats2023_index.csv\"\n",
        "df = pd.read_csv(index_path)\n",
        "\n",
        "# First column is case_id\n",
        "case_ids = df.iloc[:, 0].tolist()\n",
        "print(f\"✅ Found {len(case_ids)} BRATS2023 cases.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbKDEuOGWvgw",
        "outputId": "50658bb5-a43f-46f5-9fa9-75abf5599025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found 1251 BRATS2023 cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Cell : PyTorch Dataset & tumor-aware patch sampler\n",
        "# ================================================\n",
        "#!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
        "# (If you have GPU runtime, comment the line above and rely on built-in CUDA torch.)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.backends.cudnn.benchmark = True\n",
        "PATCH = 112\n",
        "TUMOR_CENTER_PROB = 0.6\n",
        "\n",
        "class BratsPatchDataset(Dataset):\n",
        "    def __init__(self, case_ids, cache_dir=CACHE_DIR, patch=PATCH, tumor_center_prob=TUMOR_CENTER_PROB, augment=True):\n",
        "        self.ids = case_ids\n",
        "        self.cache = Path(cache_dir)\n",
        "        self.patch = patch\n",
        "        self.tumor_center_prob = tumor_center_prob\n",
        "        self.augment = augment\n",
        "\n",
        "    def _load_case(self, cid):\n",
        "        d = self.cache / cid\n",
        "        t1c = np.load(d/\"t1c.npy\",mmap_mode='r'); t1n = np.load(d/\"t1n.npy\",mmap_mode='r')\n",
        "        t2w = np.load(d/\"t2w.npy\",mmap_mode='r'); t2f = np.load(d/\"t2f.npy\",mmap_mode='r')\n",
        "        seg = np.load(d/\"seg.npy\",mmap_mode='r')\n",
        "        vol = np.stack([t1c, t1n, t2w, t2f], axis=0) # (C,Z,Y,X)\n",
        "        return vol, seg\n",
        "\n",
        "    def _rand_center(self, seg, tumor_bias=True):\n",
        "        Z,Y,X = seg.shape\n",
        "        if tumor_bias and np.random.rand() < self.tumor_center_prob and (seg>0).any():\n",
        "            tz,ty,tx = np.array(np.where(seg>0)).T[np.random.randint((seg>0).sum())]\n",
        "        else:\n",
        "            tz,ty,tx = np.random.randint(Z), np.random.randint(Y), np.random.randint(X)\n",
        "        return tz,ty,tx\n",
        "\n",
        "    def _crop_patch(self, vol, seg, cz, cy, cx):\n",
        "        # vol: (C,Z,Y,X)\n",
        "        C,Z,Y,X = vol.shape\n",
        "        ps = self.patch\n",
        "        z0 = np.clip(cz - ps//2, 0, Z-ps); y0 = np.clip(cy - ps//2, 0, Y-ps); x0 = np.clip(cx - ps//2, 0, X-ps)\n",
        "        z1,y1,x1 = z0+ps, y0+ps, x0+ps\n",
        "        v = vol[:, z0:z1, y0:y1, x0:x1]\n",
        "        s = seg[z0:z1, y0:y1, x0:x1]\n",
        "        return v, s\n",
        "\n",
        "    def _augment(self, v, s):\n",
        "        # Simple flips + small rotations (in-plane). Keep light to start.\n",
        "        if np.random.rand()<0.5:\n",
        "            v = v[:, :, :, ::-1]; s = s[:, :, ::-1]\n",
        "        if np.random.rand()<0.5:\n",
        "            v = v[:, :, ::-1, :]; s = s[:, ::-1, :]\n",
        "        if np.random.rand()<0.5:\n",
        "            v = v[:, ::-1, :, :]; s = s[::-1, :, :]\n",
        "        # modality dropout (15%)\n",
        "        if np.random.rand()<0.15:\n",
        "            ch = np.random.randint(4); v[ch] = 0.0\n",
        "        return v, s\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cid = self.ids[idx]\n",
        "        vol, seg = self._load_case(cid)\n",
        "        cz,cy,cx = self._rand_center(seg, tumor_bias=True)\n",
        "        v, s = self._crop_patch(vol, seg, cz,cy,cx)\n",
        "        if self.augment:\n",
        "            v, s = self._augment(v, s)\n",
        "        # to tensors\n",
        "        v = torch.from_numpy(v.copy()).float()\n",
        "        s = torch.from_numpy(s.copy()).long()  # (Z,Y,X)\n",
        "        # one-hot\n",
        "        num_classes = 4\n",
        "        oh = torch.zeros((num_classes,)+s.shape, dtype=torch.float32)\n",
        "        for c in range(num_classes):\n",
        "            oh[c] = (s==c).float()\n",
        "        return dict(image=v, target=oh, case_id=cid)\n"
      ],
      "metadata": {
        "id": "qb7iMtQdJOfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai nibabel tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmVwSfuEJqDx",
        "outputId": "9b63441f-f81e-4c2d-bcaf-243f06f266af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.9.0+cu126)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Baselines: 3D UNet and nnU-Net-like (DynUNet)\n",
        "# =========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.networks.nets import UNet, DynUNet\n",
        "\n",
        "class ReturnTuple(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrap any segmentation model so that forward() returns (seg_logits, None),\n",
        "    matching your M3ANeuroSeg signature (seg_logits, bmap_logits).\n",
        "    \"\"\"\n",
        "    def __init__(self, core: nn.Module):\n",
        "        super().__init__()\n",
        "        self.core = core\n",
        "    def forward(self, x):\n",
        "        y = self.core(x)\n",
        "        return y, None\n",
        "\n",
        "def make_unet3d(in_ch=4, out_ch=4, base=32):\n",
        "    \"\"\"\n",
        "    Classic 3D U-Net (MONAI UNet). Comparable to “3D U-Net baseline”.\n",
        "    Depth: 5 levels, InstanceNorm, residual blocks for stability.\n",
        "    \"\"\"\n",
        "    model = UNet(\n",
        "        spatial_dims=3,\n",
        "        in_channels=in_ch,\n",
        "        out_channels=out_ch,\n",
        "        channels=(base, base*2, base*4, base*8, base*10),\n",
        "        strides=(2, 2, 2, 2),\n",
        "        num_res_units=2,\n",
        "        norm=\"INSTANCE\",\n",
        "        dropout=0.0\n",
        "    )\n",
        "    return ReturnTuple(model)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.networks.nets import DynUNet\n",
        "\n",
        "class DynUNetWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper to make MONAI DynUNet compatible with (seg_logits, bmap_logits) outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=4, out_ch=4, base=32):\n",
        "        super().__init__()\n",
        "        kernel_size = [\n",
        "            (3,3,3), (3,3,3), (3,3,3), (3,3,3), (3,3,3)\n",
        "        ]\n",
        "        strides = [\n",
        "            (1,1,1), (2,2,2), (2,2,2), (2,2,2), (2,2,2)\n",
        "        ]\n",
        "        upsample_kernel_size = [\n",
        "            (2,2,2), (2,2,2), (2,2,2), (2,2,2)\n",
        "        ]\n",
        "        filters = [base, base*2, base*4, base*8, base*16]\n",
        "\n",
        "        self.model = DynUNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=in_ch,\n",
        "            out_channels=out_ch,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            upsample_kernel_size=upsample_kernel_size,\n",
        "            filters=filters,\n",
        "            res_block=True,\n",
        "            norm_name=(\"instance\", {\"affine\": True}),\n",
        "            act_name=(\"leakyrelu\", {\"negative_slope\": 0.01, \"inplace\": True}),\n",
        "            dropout=0.0,\n",
        "            deep_supervision=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        seg_logits = self.model(x)\n",
        "        return seg_logits, None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7zf-jI3PHYUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9ff241-e23b-49db-cd07-3c4619972ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- helper loss components ----\n",
        "def dice_loss(logits, targets, eps=1e-5):\n",
        "    # expects logits, targets in shape (B,C,D,H,W)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    num = 2 * (probs * targets).sum(dim=(0,2,3,4))\n",
        "    den = (probs + targets).sum(dim=(0,2,3,4))\n",
        "    dice = 1 - (num + eps) / (den + eps)\n",
        "    return dice.mean()\n",
        "\n",
        "def focal_loss(logits, targets, gamma=2.0, alpha=0.25):\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    ce = F.cross_entropy(logits, targets.argmax(1), reduction='none')\n",
        "    pt = torch.exp(-ce)\n",
        "    loss = alpha * (1 - pt) ** gamma * ce\n",
        "    return loss.mean()\n",
        "\n",
        "def bce_loss(logits, targets):\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    bce = -(targets * torch.log(probs + 1e-8)).sum(dim=1)\n",
        "    return bce.mean()\n",
        "def dice_metric(y_pred, y_true, eps=1e-5):\n",
        "    # y_pred/y_true: (B,C,D,H,W) one-hot\n",
        "    dims = (0,2,3,4)\n",
        "    inter = (y_pred * y_true).sum(dim=dims)\n",
        "    denom = (y_pred + y_true).sum(dim=dims)\n",
        "    dice = (2*inter + eps) / (denom + eps)\n",
        "    return dice  # per-class\n",
        "# optional boundary loss (safe dummy version)\n",
        "def boundary_loss(bmap_logits, target):\n",
        "    \"\"\"\n",
        "    If boundary maps exist (shape = (B,1,D,H,W)), compute simple BCE.\n",
        "    Otherwise, return zero tensor.\n",
        "    \"\"\"\n",
        "    if bmap_logits is None:\n",
        "        return torch.tensor(0.0, device=target.device)\n",
        "    if bmap_logits.shape[1] != 1:\n",
        "        bmap_logits = bmap_logits.mean(dim=1, keepdim=True)\n",
        "    target_boundary = (target.sum(dim=1, keepdim=True) > 0).float()\n",
        "    return F.binary_cross_entropy_with_logits(bmap_logits, target_boundary)\n",
        "def argmax_onehot(logits):\n",
        "    arg = logits.argmax(dim=1, keepdim=True)\n",
        "    return torch.zeros_like(logits).scatter_(1, arg, 1.0)\n",
        "# ---- full unified loss ----\n",
        "def compute_loss(seg_logits, bmap_logits, target, boundary_weight=0.0, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Unified loss for M3A / UNet / nnUNet-like\n",
        "    \"\"\"\n",
        "    d = dice_loss(seg_logits, target)\n",
        "    f = focal_loss(seg_logits, target, gamma=gamma)\n",
        "    b = bce_loss(seg_logits, target)\n",
        "\n",
        "    total = d + f + b\n",
        "\n",
        "    # optional boundary component\n",
        "    if bmap_logits is not None and boundary_weight > 0:\n",
        "        bl = boundary_loss(bmap_logits, target)\n",
        "        total = total + boundary_weight * bl\n",
        "    else:\n",
        "        bl = torch.tensor(0.0, device=seg_logits.device)\n",
        "\n",
        "    return total, {\n",
        "        \"dice\": float(d.item()),\n",
        "        \"focal\": float(f.item()),\n",
        "        \"bce\": float(b.item()),\n",
        "        \"boundary\": float(bl.item())\n",
        "    }\n"
      ],
      "metadata": {
        "id": "zTkMx1uhJgQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Cell: M3A-NeuroSeg core blocks + model\n",
        "# ================================================\n",
        "from einops import rearrange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "def _partition_windows_3d(x, win):  # x: (B,C,D,H,W)\n",
        "    B,C,D,H,W = x.shape\n",
        "    wd, wh, ww = win\n",
        "    assert D%wd==0 and H%wh==0 and W%ww==0, \"D/H/W must be multiples of window size\"\n",
        "    x = x.view(B, C, D//wd, wd, H//wh, wh, W//ww, ww)\n",
        "    x = x.permute(0,2,4,6,3,5,7,1).contiguous()  # (B, nD, nH, nW, wd, wh, ww, C)\n",
        "    x = x.view(-1, wd*wh*ww, C)                  # (B*nWin, N, C)\n",
        "    return x\n",
        "\n",
        "def _reverse_windows_3d(x, win, out_shape):      # x: (B*nWin, N, C)\n",
        "    B,C,D,H,W = out_shape\n",
        "    wd, wh, ww = win\n",
        "    nD, nH, nW = D//wd, H//wh, W//ww\n",
        "    x = x.view(B, nD, nH, nW, wd, wh, ww, C)\n",
        "    x = x.permute(0,7,1,4,2,5,3,6).contiguous()  # (B, C, nD, wd, nH, wh, nW, ww)\n",
        "    x = x.view(B, C, D, H, W)\n",
        "    return x\n",
        "\n",
        "class WindowAttention3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Local 3D self-attention with non-overlapping windows.\n",
        "    Drop-in for AxialAttention3D to get real attention behavior.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads=4, window=8, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.win = (window, window, window) if isinstance(window, int) else tuple(window)\n",
        "        self.head_dim = dim // heads\n",
        "        assert dim % heads == 0, \"dim must be divisible by heads\"\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        # simple relative bias per head per token pair inside a window (optional light version)\n",
        "        N = self.win[0]*self.win[1]*self.win[2]\n",
        "        self.rel_bias = nn.Parameter(torch.zeros(heads, N, N))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,C,D,H,W)\n",
        "        B,C,D,H,W = x.shape\n",
        "        wd, wh, ww = self.win\n",
        "        # pad to multiples if needed\n",
        "        padD = (wd - D % wd) % wd\n",
        "        padH = (wh - H % wh) % wh\n",
        "        padW = (ww - W % ww) % ww\n",
        "        if padD or padH or padW:\n",
        "            x = nn.functional.pad(x, (0,padW, 0,padH, 0,padD), mode=\"constant\", value=0)\n",
        "            Dp, Hp, Wp = x.shape[2], x.shape[3], x.shape[4]\n",
        "        else:\n",
        "            Dp, Hp, Wp = D, H, W\n",
        "\n",
        "        # (B*nWin, N, C)\n",
        "        xw = _partition_windows_3d(x, self.win)\n",
        "        # project to qkv\n",
        "        qkv = self.qkv(xw)  # (B*nWin, N, 3C)\n",
        "        q,k,v = qkv.chunk(3, dim=-1)\n",
        "        # split heads: (B*nWin, heads, N, head_dim)\n",
        "        def split_heads(t): return rearrange(t, 'b n (h c) -> b h n c', h=self.heads)\n",
        "        q,k,v = split_heads(q), split_heads(k), split_heads(v)\n",
        "        q = q * self.scale\n",
        "        attn = q @ k.transpose(-2, -1)  # (B*nWin, heads, N, N)\n",
        "        attn = attn + self.rel_bias  # broadcast per head\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = attn @ v                       # (B*nWin, heads, N, head_dim)\n",
        "        out = rearrange(out, 'b h n c -> b n (h c)')\n",
        "        out = self.proj(out)                 # (B*nWin, N, C)\n",
        "        # reverse windows\n",
        "        out = _reverse_windows_3d(out, self.win, (B,C,Dp,Hp,Wp))\n",
        "        # remove pad if added\n",
        "        if (Dp,Hp,Wp)!=(D,H,W):\n",
        "            out = out[:, :, :D, :H, :W]\n",
        "        return out\n",
        "\n",
        "class LayerNorm3d(nn.Module):\n",
        "    def __init__(self, num_channels, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(1, num_channels, 1,1,1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, num_channels, 1,1,1))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        # x: (B,C,D,H,W)\n",
        "        var = x.var(dim=(2,3,4), keepdim=True, unbiased=False)\n",
        "        mean = x.mean(dim=(2,3,4), keepdim=True)\n",
        "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return x * self.weight + self.bias\n",
        "\n",
        "class ConvNeXtBlock3D(nn.Module):\n",
        "    def __init__(self, dim, drop_path=0.0):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.ln = LayerNorm3d(dim)\n",
        "        self.pw1 = nn.Conv3d(dim, 4*dim, kernel_size=1)\n",
        "        self.act = nn.GELU()\n",
        "        self.pw2 = nn.Conv3d(4*dim, dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.ones(1,dim,1,1,1))  # LayerScale\n",
        "        self.drop_path = drop_path\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.dw(x)\n",
        "        x = self.ln(x)\n",
        "        x = self.pw1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pw2(x)\n",
        "        x = self.gamma * x\n",
        "        if self.drop_path>0 and self.training:\n",
        "            if torch.rand(1).item()<self.drop_path:\n",
        "                x = torch.zeros_like(x)\n",
        "        return x + shortcut\n",
        "\n",
        "class ModalityGate(nn.Module):\n",
        "    \"\"\"Tiny Squeeze-Excite across modality channels at the input of each stage.\"\"\"\n",
        "    def __init__(self, in_ch=4, hidden=8):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc1 = nn.Conv3d(in_ch, hidden, 1)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Conv3d(hidden, in_ch, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        # x: (B,4,D,H,W)\n",
        "        g = self.pool(x)\n",
        "        g = self.fc1(g); g = self.act(g); g = self.fc2(g)\n",
        "        g = self.sig(g)\n",
        "        return x * g  # modality-wise reweight\n",
        "\n",
        "class AxialAttention3D(nn.Module):\n",
        "    \"\"\"Factorized axial attention along D, H, W with local windows (for memory).\"\"\"\n",
        "    def __init__(self, dim, heads=4, window=16):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.window = window\n",
        "        self.to_qkv = nn.Conv3d(dim, dim*3, 1)\n",
        "        self.proj = nn.Conv3d(dim, dim, 1)\n",
        "\n",
        "    def _attn_axis(self, x, axis):\n",
        "        # x: (B,C,D,H,W)\n",
        "        B,C,D,H,W = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        q,k,v = torch.chunk(qkv, 3, dim=1)\n",
        "        # split heads\n",
        "        def split_heads(t):  # (B,heads,Ch,D,H,W)\n",
        "            return rearrange(t, 'b (h c) d h1 w -> b h c d h1 w', h=self.heads)\n",
        "        q,k,v = split_heads(q), split_heads(k), split_heads(v)\n",
        "\n",
        "        # unfold windows along the chosen axis\n",
        "        if axis==2:  # D\n",
        "            win = self.window if D>=self.window else D\n",
        "            q = q.unfold(dimension=3, size=win, step=win)  # (B,h,c, D//w, w, H, W)\n",
        "            k = k.unfold(dimension=3, size=win, step=win)\n",
        "            v = v.unfold(dimension=3, size=win, step=win)\n",
        "            q = q * self.scale\n",
        "            attn = (q.transpose(-1,-3) @ k.transpose(-1,-3).transpose(-2,-1))  # not exact; keep simple\n",
        "        # For brevity we keep a simplified axial formulation.\n",
        "        # In practice, using windowed attention libs (xformers/flash-attn) is preferred.\n",
        "\n",
        "        # Fallback: return identity if axis handling too complex\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # For stability in this first skeleton, use a lightweight conv + identity skip\n",
        "        # and leave full axial attention as future optimization.\n",
        "        return x + nn.functional.conv3d(x, weight=torch.zeros((self.dim,self.dim,1,1,1), device=x.device), bias=None)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "        self.proj = nn.Conv3d(in_ch, out_ch, 1)\n",
        "    def forward(self, x): return self.proj(self.pool(x))\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)\n",
        "        self.proj = nn.Conv3d(in_ch, out_ch, 1)\n",
        "    def forward(self, x): return self.proj(self.up(x))\n",
        "\n",
        "class XScaleAttentionBridge(nn.Module):\n",
        "    \"\"\"Query high-res features (neck) with current decoder tokens (very light).\"\"\"\n",
        "    def __init__(self, ch_dec, ch_enc):\n",
        "        super().__init__()\n",
        "        self.q = nn.Conv3d(ch_dec, ch_dec, 1)\n",
        "        self.k = nn.Conv3d(ch_enc, ch_dec, 1)\n",
        "        self.v = nn.Conv3d(ch_enc, ch_dec, 1)\n",
        "        self.proj = nn.Conv3d(ch_dec, ch_dec, 1)\n",
        "    def forward(self, dec, enc):\n",
        "        # dec, enc: (B,C,D,H,W) at similar scale\n",
        "        q = self.q(dec); k = self.k(enc); v = self.v(enc)\n",
        "        attn = torch.sigmoid(q * k)  # elementwise gating (cheap)\n",
        "        out = self.proj(dec + attn * v)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BoundaryHead(nn.Module):\n",
        "    \"\"\"Predict boundary LOGITS from segmentation logits (no sigmoid here).\"\"\"\n",
        "    def __init__(self, in_ch=4):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv3d(in_ch, 1, 3, padding=1)\n",
        "    def forward(self, logits):\n",
        "        # logits: (B,4,D,H,W) -> return boundary logits (B,1,D,H,W)\n",
        "        return self.conv(logits)\n",
        "\n",
        "class M3ANeuroSeg(nn.Module):\n",
        "    def __init__(self, in_ch=4, num_classes=4, dims=(32, 64, 128, 256)):\n",
        "        super().__init__()\n",
        "        self.gate = ModalityGate(in_ch, hidden=8)\n",
        "        self.stem = nn.Conv3d(in_ch, dims[0], kernel_size=3, padding=1)\n",
        "\n",
        "        # Encoder dual-path (ConvNeXt local + axial/global placeholder)\n",
        "        self.enc1_local = ConvNeXtBlock3D(dims[0]); self.enc1_global = WindowAttention3D(dims[0], heads=4, window=8)\n",
        "        self.down1 = Down(dims[0], dims[1])\n",
        "\n",
        "        self.enc2_local = ConvNeXtBlock3D(dims[1]); self.enc2_global = WindowAttention3D(dims[1], heads=4, window=8)\n",
        "        self.down2 = Down(dims[1], dims[2])\n",
        "\n",
        "        self.enc3_local = ConvNeXtBlock3D(dims[2]); self.enc3_global = WindowAttention3D(dims[2], heads=4, window=8)\n",
        "        self.down3 = Down(dims[2], dims[3])\n",
        "\n",
        "        self.enc4_local = ConvNeXtBlock3D(dims[3]); self.enc4_global = WindowAttention3D(dims[3], heads=4, window=8)\n",
        "\n",
        "        # Neck (simple fusion; deformable skipped in skeleton)\n",
        "        self.neck = nn.Conv3d(dims[3], dims[3], 1)\n",
        "\n",
        "        # Decoder without raw U-skips; use bridges\n",
        "        self.up3 = Up(dims[3], dims[2]); self.bridge3 = XScaleAttentionBridge(dims[2], dims[2])\n",
        "        self.dec3 = ConvNeXtBlock3D(dims[2])\n",
        "\n",
        "        self.up2 = Up(dims[2], dims[1]); self.bridge2 = XScaleAttentionBridge(dims[1], dims[1])\n",
        "        self.dec2 = ConvNeXtBlock3D(dims[1])\n",
        "\n",
        "        self.up1 = Up(dims[1], dims[0]); self.bridge1 = XScaleAttentionBridge(dims[0], dims[0])\n",
        "        self.dec1 = ConvNeXtBlock3D(dims[0])\n",
        "\n",
        "        self.head = nn.Conv3d(dims[0], num_classes, 1)\n",
        "        self.boundary = BoundaryHead(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.gate(x)\n",
        "      x0 = self.stem(x)\n",
        "      e1 = self.enc1_local(x0) + self.enc1_global(x0)\n",
        "      x1 = self.down1(e1)\n",
        "      e2 = self.enc2_local(x1) + self.enc2_global(x1)\n",
        "      x2 = self.down2(e2)\n",
        "      e3 = self.enc3_local(x2) + self.enc3_global(x2)\n",
        "      x3 = self.down3(e3)\n",
        "      e4 = self.enc4_local(x3) + self.enc4_global(x3)\n",
        "      neck = self.neck(e4)\n",
        "\n",
        "      d3 = self.up3(neck); d3 = self.bridge3(d3, e3); d3 = self.dec3(d3)\n",
        "      d2 = self.up2(d3);   d2 = self.bridge2(d2, e2); d2 = self.dec2(d2)\n",
        "      d1 = self.up1(d2);   d1 = self.bridge1(d1, e1); d1 = self.dec1(d1)\n",
        "\n",
        "      logits = self.head(d1)              # (B,4,D,H,W) class logits\n",
        "      bmap_logits = self.boundary(logits) # (B,1,D,H,W) boundary logits (no sigmoid)\n",
        "      return logits, bmap_logits\n",
        "\n",
        "\n",
        "# Sanity dry-run\n",
        "model = M3ANeuroSeg(in_ch=4, num_classes=4).cpu()\n",
        "x = torch.randn(1,4,128,128,128)\n",
        "with torch.no_grad():\n",
        "    lo, bm = model(x)\n",
        "print(\"Model OK. logits:\", lo.shape, \"bmap:\", bm.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7BBBqCw96Yk",
        "outputId": "3b5b99a5-2c27-4735-a96e-7219c822882f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model OK. logits: torch.Size([1, 4, 128, 128, 128]) bmap: torch.Size([1, 1, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Tri-Model Ensemble (UNet3D + DynUNet + M3A) — Mode A (per-fold, out-of-fold)\n",
        "# ================================================\n",
        "import os, json, gc, time\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
        "\n",
        "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PATCH   = globals().get(\"PATCH\", 112)\n",
        "OVERLAP = globals().get(\"OVERLAP\", 0.5)\n",
        "\n",
        "# --------- helpers ---------\n",
        "def load_case_4ch_seg(case_id):\n",
        "    d = Path(CACHE_DIR) / case_id\n",
        "    vols = np.stack([\n",
        "        np.load(d/\"t1c.npy\"),\n",
        "        np.load(d/\"t1n.npy\"),\n",
        "        np.load(d/\"t2w.npy\"),\n",
        "        np.load(d/\"t2f.npy\"),\n",
        "    ], axis=0)  # (4, Z, Y, X)\n",
        "    seg = np.load(d/\"seg.npy\").astype(np.int64)      # (Z, Y, X) labels: 0 bg, 1 NCR, 2 ED, 4 ET\n",
        "    return vols, seg\n",
        "\n",
        "def one_hot_from_labels(lbl, num_classes):\n",
        "    # lbl shape: (1,1,Z,Y,X) int64\n",
        "    b, c, *sp = lbl.shape\n",
        "    oh = torch.zeros((b, num_classes, *sp), dtype=torch.float32, device=lbl.device)\n",
        "    for k in range(num_classes):\n",
        "        oh[:, k] = (lbl[:,0] == k)\n",
        "    return oh\n",
        "\n",
        "def compute_composites_onehot(pred_bool, gt_bool):\n",
        "    # pred_bool/gt_bool: (B,C,Z,Y,X) boolean one-hots for classes [0,BG,1 NCR, 2 ED, 3 ?], here we map from labels {0,1,2,4}\n",
        "    # We assume channels are [0:bg,1:NCR,2:ED,3:ET] — ensure your model maps ET→channel 3 in training.\n",
        "    # (That’s how your training/eval pipeline was set up.)\n",
        "    bg  = pred_bool[:,0]\n",
        "    ncr = pred_bool[:,1]\n",
        "    ed  = pred_bool[:,2]\n",
        "    et  = pred_bool[:,3]\n",
        "\n",
        "    bg_g  = gt_bool[:,0]\n",
        "    ncr_g = gt_bool[:,1]\n",
        "    ed_g  = gt_bool[:,2]\n",
        "    et_g  = gt_bool[:,3]\n",
        "\n",
        "    # ET\n",
        "    ET_pred = et.unsqueeze(1).float()\n",
        "    ET_true = et_g.unsqueeze(1).float()\n",
        "\n",
        "    # TC = NCR ∪ ET\n",
        "    TC_pred = ((ncr | et).unsqueeze(1)).float()\n",
        "    TC_true = ((ncr_g | et_g).unsqueeze(1)).float()\n",
        "\n",
        "    # WT = NCR ∪ ED ∪ ET\n",
        "    WT_pred = ((ncr | ed | et).unsqueeze(1)).float()\n",
        "    WT_true = ((ncr_g | ed_g | et_g).unsqueeze(1)).float()\n",
        "\n",
        "    return {\"ET\": (ET_pred, ET_true), \"TC\": (TC_pred, TC_true), \"WT\": (WT_pred, WT_true)}\n",
        "\n",
        "@torch.no_grad()\n",
        "def sw_logits(model, vol4, roi=112, overlap=0.5):\n",
        "    # vol4: (4, Z, Y, X) numpy\n",
        "    x = torch.from_numpy(vol4[None].copy()).float().to(device).to(memory_format=torch.channels_last_3d)\n",
        "    with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "        def predictor(t):\n",
        "            out = model(t)\n",
        "            # unify to seg_logits if model returns (logits, None)\n",
        "            if isinstance(out, (tuple, list)):\n",
        "                return out[0]\n",
        "            return out\n",
        "        logits = sliding_window_inference(\n",
        "            inputs=x, roi_size=(roi,roi,roi), overlap=overlap,\n",
        "            sw_batch_size=1, predictor=predictor, mode=\"gaussian\"\n",
        "        )\n",
        "    return logits[0].float().cpu().numpy()  # (C, Z, Y, X)\n",
        "\n",
        "def make_m3a():       return M3ANeuroSeg(in_ch=4, num_classes=4, dims=(32,64,128,256))\n",
        "def make_unet():      return make_unet3d(in_ch=4, out_ch=4, base=32)\n",
        "def make_dynunet():   return DynUNetWrapper(in_ch=4, out_ch=4, base=32)\n",
        "\n",
        "def load_model(ctor, ckpt_path):\n",
        "    m = ctor().to(device).to(memory_format=torch.channels_last_3d)\n",
        "    sd = torch.load(ckpt_path, map_location=device)[\"model\"]\n",
        "    m.load_state_dict(sd)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "def get_val_ids_for_fold(fold):\n",
        "    with open(f\"{IDX_DIR}/splits_5fold_seed{SEED}.json\",\"r\") as f:\n",
        "        return list(json.load(f)[\"splits\"][f\"fold_{fold}\"])\n",
        "\n",
        "# --------- main tri-ensemble eval for one fold ---------\n",
        "def eval_fold_tri_ensemble(fold, tta=False, save_csv=True):\n",
        "    # resolve checkpoints (adjust names if your folders differ)\n",
        "    ckpt_unet = f\"/content/drive/MyDrive/m3a_neuroseg/runs/unet3d_brats2023_fold4_cycle1/best.pt\"\n",
        "    ckpt_dyn  = f\"/content/drive/MyDrive/m3a_neuroseg/runs/nnunet_like_brats2023_fold4_cycle1/best.pt\"\n",
        "    ckpt_m3a  = f\"/content/drive/MyDrive/m3a_neuroseg/runs/m3a_brats2023_fold4_cycle1/best.pt\"\n",
        "    assert os.path.exists(ckpt_unet) and os.path.exists(ckpt_dyn) and os.path.exists(ckpt_m3a), \"Missing one or more checkpoints.\"\n",
        "\n",
        "    # load once\n",
        "    model_unet = load_model(make_unet, ckpt_unet)\n",
        "    model_dyn  = load_model(make_dynunet, ckpt_dyn)\n",
        "    model_m3a  = load_model(make_m3a, ckpt_m3a)\n",
        "\n",
        "    # metrics\n",
        "    dice_fn = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
        "    hd95_fn = HausdorffDistanceMetric(include_background=False, reduction=\"mean_batch\", percentile=95)\n",
        "    dice_fn.reset(); hd95_fn.reset()\n",
        "\n",
        "    rows = []\n",
        "    val_ids = get_val_ids_for_fold(fold)\n",
        "\n",
        "    for cid in tqdm(val_ids, desc=f\"Fold {fold} tri-ensemble\", ncols=110):\n",
        "        vol, gt = load_case_4ch_seg(cid)\n",
        "\n",
        "        # base logits\n",
        "        L1 = sw_logits(model_unet, vol, roi=PATCH, overlap=OVERLAP)\n",
        "        L2 = sw_logits(model_dyn,  vol, roi=PATCH, overlap=OVERLAP)\n",
        "        L3 = sw_logits(model_m3a,  vol, roi=PATCH, overlap=OVERLAP)\n",
        "        logits = (L1 + L2 + L3) / 3.0\n",
        "\n",
        "        # (optional) simple flip-TTA on Z axis for a small boost\n",
        "        if tta:\n",
        "            vol_flip = vol[:, ::-1]  # flip Z\n",
        "            L1f = sw_logits(model_unet, vol_flip, roi=PATCH, overlap=OVERLAP)[:, ::-1]\n",
        "            L2f = sw_logits(model_dyn,  vol_flip, roi=PATCH, overlap=OVERLAP)[:, ::-1]\n",
        "            L3f = sw_logits(model_m3a,  vol_flip, roi=PATCH, overlap=OVERLAP)[:, ::-1]\n",
        "            logits = 0.5 * logits + 0.5 * ((L1f + L2f + L3f) / 3.0)\n",
        "\n",
        "        # to tensors for metrics\n",
        "        pred_lbl = torch.from_numpy(logits).unsqueeze(0).argmax(1, keepdim=True)  # (1,1,Z,Y,X)\n",
        "        pred_oh  = one_hot_from_labels(pred_lbl.to(torch.int64), 4).float().to(device)\n",
        "        gt_oh    = one_hot_from_labels(torch.from_numpy(gt)[None,None].to(device), 4).float()\n",
        "\n",
        "        dice_fn(y_pred=pred_oh, y=gt_oh)\n",
        "        hd95_fn(y_pred=pred_oh, y=gt_oh)\n",
        "\n",
        "        # composites\n",
        "        comps = compute_composites_onehot(pred_oh.bool(), gt_oh.bool())\n",
        "        def _dice(y_pred, y_true):\n",
        "            m = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "            return float(m(y_pred=y_pred.float(), y=y_true.float()).item())\n",
        "        def _hd95(y_pred, y_true):\n",
        "            m = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
        "            return float(m(y_pred=y_pred.float(), y=y_true.float()).item())\n",
        "\n",
        "        dET = _dice(*comps[\"ET\"]); dTC = _dice(*comps[\"TC\"]); dWT = _dice(*comps[\"WT\"])\n",
        "        hET = _hd95(*comps[\"ET\"]); hTC = _hd95(*comps[\"TC\"]); hWT = _hd95(*comps[\"WT\"])\n",
        "\n",
        "        rows.append(dict(case_id=cid, dice_ET=dET, dice_TC=dTC, dice_WT=dWT,\n",
        "                         hd95_ET=hET, hd95_TC=hTC, hd95_WT=hWT))\n",
        "\n",
        "        del L1, L2, L3, logits, pred_lbl, pred_oh, gt_oh\n",
        "        gc.collect()\n",
        "\n",
        "    d = np.squeeze(dice_fn.aggregate().cpu().numpy()).astype(np.float32)  # [NCR,ED,ET]\n",
        "    h = np.squeeze(hd95_fn.aggregate().cpu().numpy()).astype(np.float32)\n",
        "\n",
        "    print(f\"\\nFold {fold} (tri-ensemble {'+TTA' if tta else 'no-TTA'})\")\n",
        "    print(f\"  Dice_mean (NCR/ED/ET): {d.mean():.4f} | {np.round(d,4)}\")\n",
        "    print(f\"  HD95_mean (mm): {h.mean():.2f} | {np.round(h,2)}\")\n",
        "\n",
        "    import pandas as pd\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    print(f\"  Composites (mean): ET={out_df['dice_ET'].mean():.4f}, TC={out_df['dice_TC'].mean():.4f}, WT={out_df['dice_WT'].mean():.4f}\")\n",
        "    print(f\"  HD95 comps (mm):  ET={out_df['hd95_ET'].mean():.2f}, TC={out_df['hd95_TC'].mean():.2f}, WT={out_df['hd95_WT'].mean():.2f}\")\n",
        "\n",
        "    if save_csv:\n",
        "        out_dir = f\"/content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        if fold!=4:\n",
        "          out_df.to_csv(f\"{out_dir}/fold{fold}_tri_ensemble_cases.csv\", index=False)\n",
        "          print(\"  ✓ Saved:\", f\"{out_dir}/fold{fold}_tri_ensemble_cases.csv\")\n",
        "        else:\n",
        "          out_df.to_csv(f\"{out_dir}/fold{fold}_newtri_ensemble_cases.csv\", index=False)\n",
        "          print(\"  ✓ Saved:\", f\"{out_dir}/fold{fold}_newtri_ensemble_cases.csv\")\n",
        "    return out_df, d, h\n",
        "\n",
        "# --------- RUN: pick the fold and go ---------\n",
        "#fold = 4   # <-- set 0..4\n",
        "#_ = eval_fold_tri_ensemble(fold, tta=False, save_csv=True)\n",
        "def eval_all_folds(tta=False):\n",
        "    all_rows = []\n",
        "    all_native_dice = []\n",
        "    all_native_hd95 = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\" Evaluating Fold {fold}\")\n",
        "        print(f\"==============================\")\n",
        "\n",
        "        out_df, d, h = eval_fold_tri_ensemble(fold, tta=tta, save_csv=True)\n",
        "\n",
        "        # collect case-level composite rows\n",
        "        all_rows.append(out_df)\n",
        "\n",
        "        # collect class-wise native metrics\n",
        "        all_native_dice.append(d)\n",
        "        all_native_hd95.append(h)\n",
        "\n",
        "    # merge dataframes\n",
        "    import pandas as pd\n",
        "    final_df = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    # aggregate native metrics\n",
        "    all_native_dice = np.vstack(all_native_dice)\n",
        "    all_native_hd95 = np.vstack(all_native_hd95)\n",
        "\n",
        "    mean_dice = all_native_dice.mean(axis=0)\n",
        "    mean_hd95 = all_native_hd95.mean(axis=0)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" Final Tri-Ensemble Metrics Across ALL Folds\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    print(f\"Native Dice (NCR/ED/ET): {mean_dice.mean():.4f} | {np.round(mean_dice,4)}\")\n",
        "    print(f\"Native HD95 (mm): {mean_hd95.mean():.2f} | {np.round(mean_hd95,2)}\")\n",
        "\n",
        "    # composite metrics\n",
        "    print(\"\\nComposite Region Means:\")\n",
        "    print(f\"ET Dice: {final_df['dice_ET'].mean():.4f}\")\n",
        "    print(f\"TC Dice: {final_df['dice_TC'].mean():.4f}\")\n",
        "    print(f\"WT Dice: {final_df['dice_WT'].mean():.4f}\")\n",
        "\n",
        "    print(\"\\nComposite Region HD95 (mm):\")\n",
        "    print(f\"ET HD95: {final_df['hd95_ET'].mean():.2f}\")\n",
        "    print(f\"TC HD95: {final_df['hd95_TC'].mean():.2f}\")\n",
        "    print(f\"WT HD95: {final_df['hd95_WT'].mean():.2f}\")\n",
        "\n",
        "    # save full summary\n",
        "    final_df.to_csv(\n",
        "        \"/content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results/all_folds_tri_ensemble_cases.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(\"\\n✓ Saved full-case evaluation for all 5 folds.\")\n",
        "    return final_df, mean_dice, mean_hd95\n",
        "_ = eval_all_folds(tta=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxhA_1QA9s3R",
        "outputId": "381a129c-16b1-46bd-e2bc-25a016b51fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Evaluating Fold 0\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFold 0 tri-ensemble:   0%|                                                            | 0/252 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:231: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  win_data = inputs[unravel_slice[0]].to(sw_device)\n",
            "/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  out[idx_zm] += p\n",
            "/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "Fold 0 tri-ensemble:   0%|▏                                                 | 1/252 [00:38<2:42:35, 38.87s/it]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 0 is all 0, this may result in nan/inf distance.\n",
            "  warnings.warn(\n",
            "Fold 0 tri-ensemble:   1%|▍                                                 | 2/252 [00:56<1:50:35, 26.54s/it]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 0 is all 0, this may result in nan/inf distance.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n",
            "  warnings.warn(\n",
            "Fold 0 tri-ensemble:   3%|█▍                                                  | 7/252 [01:55<50:49, 12.45s/it]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
            "  warnings.warn(\n",
            "Fold 0 tri-ensemble: 100%|██████████████████████████████████████████████████| 252/252 [34:42<00:00,  8.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 0 (tri-ensemble +TTA)\n",
            "  Dice_mean (NCR/ED/ET): 0.8843 | [0.8469 0.8995 0.9066]\n",
            "  HD95_mean (mm): 3.18 | [3.66 3.06 2.81]\n",
            "  Composites (mean): ET=0.9066, TC=0.9434, WT=0.9519\n",
            "  HD95 comps (mm):  ET=2.81, TC=2.80, WT=2.93\n",
            "  ✓ Saved: /content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results/fold0_tri_ensemble_cases.csv\n",
            "\n",
            "==============================\n",
            " Evaluating Fold 1\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 tri-ensemble: 100%|██████████████████████████████████████████████████| 252/252 [34:13<00:00,  8.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1 (tri-ensemble +TTA)\n",
            "  Dice_mean (NCR/ED/ET): 0.8689 | [0.8375 0.8898 0.8794]\n",
            "  HD95_mean (mm): 3.71 | [3.74 4.04 3.34]\n",
            "  Composites (mean): ET=0.8794, TC=0.9292, WT=0.9439\n",
            "  HD95 comps (mm):  ET=3.34, TC=3.26, WT=3.74\n",
            "  ✓ Saved: /content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results/fold1_tri_ensemble_cases.csv\n",
            "\n",
            "==============================\n",
            " Evaluating Fold 2\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 tri-ensemble: 100%|██████████████████████████████████████████████████| 249/249 [35:59<00:00,  8.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 2 (tri-ensemble +TTA)\n",
            "  Dice_mean (NCR/ED/ET): 0.8738 | [0.834  0.88   0.9074]\n",
            "  HD95_mean (mm): 3.57 | [3.89 4.74 2.09]\n",
            "  Composites (mean): ET=0.9074, TC=0.9382, WT=0.9448\n",
            "  HD95 comps (mm):  ET=2.09, TC=2.85, WT=4.51\n",
            "  ✓ Saved: /content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results/fold2_tri_ensemble_cases.csv\n",
            "\n",
            "==============================\n",
            " Evaluating Fold 3\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 tri-ensemble: 100%|██████████████████████████████████████████████████| 249/249 [33:09<00:00,  7.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 3 (tri-ensemble +TTA)\n",
            "  Dice_mean (NCR/ED/ET): 0.8829 | [0.8432 0.8895 0.916 ]\n",
            "  HD95_mean (mm): 3.65 | [3.4  5.04 2.51]\n",
            "  Composites (mean): ET=0.9160, TC=0.9488, WT=0.9466\n",
            "  HD95 comps (mm):  ET=2.51, TC=2.96, WT=4.60\n",
            "  ✓ Saved: /content/drive/MyDrive/m3a_neuroseg/runs/tri_ensemble_results/fold3_tri_ensemble_cases.csv\n",
            "\n",
            "==============================\n",
            " Evaluating Fold 4\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 tri-ensemble:  76%|█████████████████████████████████████▊            | 188/249 [28:55<06:53,  6.78s/it]"
          ]
        }
      ]
    }
  ]
}